{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considering 25% of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the source and destination directories\n",
    "source_dir = 'train2014'\n",
    "destination_dir = 'Quarter_Dataset'\n",
    "\n",
    "# Create the destination directory if it doesn't exist\n",
    "if not os.path.exists(destination_dir):\n",
    "    os.makedirs(destination_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get a list of all files in the source directory\n",
    "all_files = os.listdir(source_dir)\n",
    "\n",
    "# Filter out non-image files (optional, assuming your images are in common formats like .jpg, .png)\n",
    "image_files = [f for f in all_files if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n",
    "\n",
    "# Calculate 25% of the total number of image files\n",
    "num_files_to_copy = len(image_files) // 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 20695 files to Quarter_Dataset\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Randomly select 25% of the image files\n",
    "selected_files = random.sample(image_files, num_files_to_copy)\n",
    "\n",
    "# Copy the selected files to the destination directory\n",
    "for file_name in selected_files:\n",
    "    src_file = os.path.join(source_dir, file_name)\n",
    "    dst_file = os.path.join(destination_dir, file_name)\n",
    "    shutil.copy(src_file, dst_file)\n",
    "\n",
    "print(f\"Copied {num_files_to_copy} files to {destination_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering the corresponding questions and annotations from the json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_json_by_image_ids(dataset_dir, input_json_path, output_json_path=None):\n",
    "    # Create the output JSON filename if not provided\n",
    "    if output_json_path is None:\n",
    "        output_json_path = f'Filtered_{os.path.basename(input_json_path)}'\n",
    "    \n",
    "    # Read the JSON file\n",
    "    with open(input_json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Ensure data has the correct structure\n",
    "    if \"questions\" not in data:\n",
    "        raise ValueError(\"The JSON file does not have a 'questions' key.\")\n",
    "    \n",
    "    questions = data[\"questions\"]\n",
    "\n",
    "    # Define the set of image extensions to consider\n",
    "    image_extensions = ('.png', '.jpg', '.jpeg', '.gif', '.bmp')\n",
    "    \n",
    "    # Extract image IDs from the filenames in the dataset folder\n",
    "    image_ids = set()\n",
    "    pattern = re.compile(r'COCO_train2014_(\\d+)\\.(png|jpg|jpeg|gif|bmp)', re.IGNORECASE)\n",
    "    for filename in os.listdir(dataset_dir):\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            image_ids.add(int(match.group(1)))\n",
    "\n",
    "    # Filter the JSON entries\n",
    "    filtered_data = [entry for entry in questions if entry['image_id'] in image_ids]\n",
    "\n",
    "    # Write the filtered entries to the output JSON file\n",
    "    with open(output_json_path, 'w') as file:\n",
    "        json.dump({\"questions\": filtered_data}, file, indent=4)\n",
    "\n",
    "    print(f\"Filtered JSON entries have been written to {output_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered JSON entries have been written to Filtered_jsons/Questions.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "dataset_dir = \"Quarter_Dataset\"\n",
    "input_json_path = \"v2_OpenEnded_mscoco_train2014_questions.json\"  # Replace with your actual JSON file name\n",
    "output_json_path = \"Filtered_jsons/Questions.json\"  # Optional: Replace with desired output JSON file path if needed\n",
    "\n",
    "filter_json_by_image_ids(dataset_dir, input_json_path, output_json_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_annotations_by_image_ids(dataset_dir, input_json_path, output_json_path=None):\n",
    "    # Create the output JSON filename if not provided\n",
    "    if output_json_path is None:\n",
    "        output_json_path = f'Filtered_{os.path.basename(input_json_path)}'\n",
    "    \n",
    "    # Read the JSON file\n",
    "    with open(input_json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Ensure data has the correct structure\n",
    "    if \"annotations\" not in data:\n",
    "        raise ValueError(\"The JSON file does not have an 'annotations' key.\")\n",
    "    \n",
    "    annotations = data[\"annotations\"]\n",
    "\n",
    "    # Define the set of image extensions to consider\n",
    "    image_extensions = ('.png', '.jpg', '.jpeg', '.gif', '.bmp')\n",
    "    \n",
    "    # Extract image IDs from the filenames in the dataset folder\n",
    "    image_ids = set()\n",
    "    pattern = re.compile(r'COCO_train2014_(\\d+)\\.(png|jpg|jpeg|gif|bmp)', re.IGNORECASE)\n",
    "    for filename in os.listdir(dataset_dir):\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            image_ids.add(int(match.group(1)))\n",
    "\n",
    "    # Filter the JSON entries\n",
    "    filtered_annotations = [entry for entry in annotations if entry['image_id'] in image_ids]\n",
    "\n",
    "    # Write the filtered entries to the output JSON file\n",
    "    with open(output_json_path, 'w') as file:\n",
    "        json.dump({\"annotations\": filtered_annotations}, file, indent=4)\n",
    "\n",
    "    print(f\"Filtered JSON entries have been written to {output_json_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered JSON entries have been written to Filtered_jsons/Annotations.json\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "dataset_dir = 'Quarter_Dataset'\n",
    "input_json_path = 'v2_mscoco_train2014_annotations.json'  # Replace with your actual JSON file name\n",
    "output_json_path = 'Filtered_jsons/Annotations.json'  # Optional: Replace with desired output JSON file path if needed\n",
    "\n",
    "filter_annotations_by_image_ids(dataset_dir, input_json_path, output_json_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Json files to CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_ids have been written to train_img_list.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Define the path for the JSON file and the output text file\n",
    "json_file_path = 'Filtered_jsons/Questions.json'\n",
    "output_dir = 'Final_dataset'\n",
    "output_file_path = os.path.join(output_dir, 'train_image_ids.txt')\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    # Load the JSON data from the file\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "question_ids = [str(question['question_id']) for question in data['questions']]\n",
    "\n",
    "# Write the question_ids to the train_img_list.txt file\n",
    "with open('train_img_list.txt', 'w') as file:\n",
    "    for question_id in question_ids:\n",
    "        file.write(question_id + '\\n')\n",
    "\n",
    "print(\"question_ids have been written to train_img_list.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "# Load annotations.json\n",
    "with open('Filtered_jsons/Annotations.json', 'r') as f:\n",
    "    annotations_data = json.load(f)\n",
    "\n",
    "# Load questions.json\n",
    "with open('Filtered_jsons/Questions.json', 'r') as f:\n",
    "    questions_data = json.load(f)\n",
    "\n",
    "# Create a dictionary to map question_id to question text\n",
    "question_id_to_text = {question['question_id']: question['question'] for question in questions_data['questions']}\n",
    "\n",
    "# Create a list to store rows for the CSV\n",
    "csv_data = []\n",
    "\n",
    "# Iterate through annotations to extract relevant information\n",
    "for annotation in annotations_data['annotations']:\n",
    "    question_id = annotation['question_id']\n",
    "    # Look up the corresponding question text using the question_id\n",
    "    question = question_id_to_text.get(question_id, '')\n",
    "    # Extract the multiple choice answer and image_id\n",
    "    answer = annotation['multiple_choice_answer']\n",
    "    image_id = annotation['image_id']\n",
    "    # Append the data to the CSV data list\n",
    "    csv_data.append([question, answer, image_id])\n",
    "\n",
    "# Write the CSV file\n",
    "with open('data_train.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Question', 'Answer', 'Image_id'])  # Write header\n",
    "    writer.writerows(csv_data)\n",
    "\n",
    "print(\"CSV file created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the Validation dataset in a similar fashiom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the source and destination directories\n",
    "source_dir = 'val2014'\n",
    "destination_dir = 'Validation_Quarter_Dataset'\n",
    "\n",
    "# Create the destination directory if it doesn't exist\n",
    "if not os.path.exists(destination_dir):\n",
    "    os.makedirs(destination_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 10126 files to Validation_Quarter_Dataset\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get a list of all files in the source directory\n",
    "all_files = os.listdir(source_dir)\n",
    "\n",
    "# Filter out non-image files (optional, assuming your images are in common formats like .jpg, .png)\n",
    "image_files = [f for f in all_files if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n",
    "\n",
    "# Calculate 25% of the total number of image files\n",
    "num_files_to_copy = len(image_files) // 4\n",
    "\n",
    "\n",
    "# Randomly select 25% of the image files\n",
    "selected_files = random.sample(image_files, num_files_to_copy)\n",
    "\n",
    "# Copy the selected files to the destination directory\n",
    "for file_name in selected_files:\n",
    "    src_file = os.path.join(source_dir, file_name)\n",
    "    dst_file = os.path.join(destination_dir, file_name)\n",
    "    shutil.copy(src_file, dst_file)\n",
    "\n",
    "print(f\"Copied {num_files_to_copy} files to {destination_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_json_by_image_ids(dataset_dir, input_json_path, output_json_path=None):\n",
    "    # Create the output JSON filename if not provided\n",
    "    if output_json_path is None:\n",
    "        output_json_path = f'Filtered_{os.path.basename(input_json_path)}'\n",
    "    \n",
    "    # Read the JSON file\n",
    "    with open(input_json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Ensure data has the correct structure\n",
    "    if \"questions\" not in data:\n",
    "        raise ValueError(\"The JSON file does not have a 'questions' key.\")\n",
    "    \n",
    "    questions = data[\"questions\"]\n",
    "\n",
    "    # Define the set of image extensions to consider\n",
    "    image_extensions = ('.png', '.jpg', '.jpeg', '.gif', '.bmp')\n",
    "    \n",
    "    # Extract image IDs from the filenames in the dataset folder\n",
    "    image_ids = set()\n",
    "    pattern = re.compile(r'COCO_val2014_(\\d+)\\.(png|jpg|jpeg|gif|bmp)', re.IGNORECASE)\n",
    "    for filename in os.listdir(dataset_dir):\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            image_ids.add(int(match.group(1)))\n",
    "\n",
    "    # Filter the JSON entries\n",
    "    filtered_data = [entry for entry in questions if entry['image_id'] in image_ids]\n",
    "\n",
    "    # Write the filtered entries to the output JSON file\n",
    "    with open(output_json_path, 'w') as file:\n",
    "        json.dump({\"questions\": filtered_data}, file, indent=4)\n",
    "\n",
    "    print(f\"Filtered JSON entries have been written to {output_json_path}\")\n",
    "\n",
    "# Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered JSON entries have been written to Filtered_jsons/Val_Questions.json\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = \"Validation_Quarter_Dataset\"\n",
    "input_json_path = \"v2_OpenEnded_mscoco_val2014_questions.json\"  # Replace with your actual JSON file name\n",
    "output_json_path = \"Filtered_jsons/Val_Questions.json\"  # Optional: Replace with desired output JSON file path if needed\n",
    "\n",
    "filter_json_by_image_ids(dataset_dir, input_json_path, output_json_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_annotations_by_image_ids(dataset_dir, input_json_path, output_json_path=None):\n",
    "    # Create the output JSON filename if not provided\n",
    "    if output_json_path is None:\n",
    "        output_json_path = f'Filtered_{os.path.basename(input_json_path)}'\n",
    "    \n",
    "    # Read the JSON file\n",
    "    with open(input_json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Ensure data has the correct structure\n",
    "    if \"annotations\" not in data:\n",
    "        raise ValueError(\"The JSON file does not have an 'annotations' key.\")\n",
    "    \n",
    "    annotations = data[\"annotations\"]\n",
    "\n",
    "    # Define the set of image extensions to consider\n",
    "    image_extensions = ('.png', '.jpg', '.jpeg', '.gif', '.bmp')\n",
    "    \n",
    "    # Extract image IDs from the filenames in the dataset folder\n",
    "    image_ids = set()\n",
    "    pattern = re.compile(r'COCO_val2014_(\\d+)\\.(png|jpg|jpeg|gif|bmp)', re.IGNORECASE)\n",
    "    for filename in os.listdir(dataset_dir):\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            image_ids.add(int(match.group(1)))\n",
    "\n",
    "    # Filter the JSON entries\n",
    "    filtered_annotations = [entry for entry in annotations if entry['image_id'] in image_ids]\n",
    "\n",
    "    # Write the filtered entries to the output JSON file\n",
    "    with open(output_json_path, 'w') as file:\n",
    "        json.dump({\"annotations\": filtered_annotations}, file, indent=4)\n",
    "\n",
    "    print(f\"Filtered JSON entries have been written to {output_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered JSON entries have been written to Filtered_jsons/Val_Annotations.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Example usage\n",
    "dataset_dir = 'Validation_Quarter_Dataset'\n",
    "input_json_path = 'v2_mscoco_val2014_annotations.json'  # Replace with your actual JSON file name\n",
    "output_json_path = 'Filtered_jsons/Val_Annotations.json'  # Optional: Replace with desired output JSON file path if needed\n",
    "\n",
    "filter_annotations_by_image_ids(dataset_dir, input_json_path, output_json_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_ids have been written to val_img_list.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Define the path for the JSON file and the output text file\n",
    "json_file_path = 'Filtered_jsons/Val_Questions.json'\n",
    "output_dir = 'Final_dataset'\n",
    "output_file_path = os.path.join(output_dir, 'val_image_ids.txt')\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    # Load the JSON data from the file\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "question_ids = [str(question['question_id']) for question in data['questions']]\n",
    "\n",
    "# Write the question_ids to the train_img_list.txt file\n",
    "with open('val_img_list.txt', 'w') as file:\n",
    "    for question_id in question_ids:\n",
    "        file.write(question_id + '\\n')\n",
    "\n",
    "print(\"question_ids have been written to val_img_list.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# Load annotations.json\n",
    "with open('Filtered_jsons/Val_Annotations.json', 'r') as f:\n",
    "    annotations_data = json.load(f)\n",
    "\n",
    "# Load questions.json\n",
    "with open('Filtered_jsons/Val_Questions.json', 'r') as f:\n",
    "    questions_data = json.load(f)\n",
    "\n",
    "# Create a dictionary to map question_id to question text\n",
    "question_id_to_text = {question['question_id']: question['question'] for question in questions_data['questions']}\n",
    "\n",
    "# Create a list to store rows for the CSV\n",
    "csv_data = []\n",
    "\n",
    "# Iterate through annotations to extract relevant information\n",
    "for annotation in annotations_data['annotations']:\n",
    "    question_id = annotation['question_id']\n",
    "    # Look up the corresponding question text using the question_id\n",
    "    question = question_id_to_text.get(question_id, '')\n",
    "    # Extract the multiple choice answer and image_id\n",
    "    answer = annotation['multiple_choice_answer']\n",
    "    image_id = annotation['image_id']\n",
    "    # Append the data to the CSV data list\n",
    "    csv_data.append([question, answer, image_id])\n",
    "\n",
    "# Write the CSV file\n",
    "with open('data_eval.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Question', 'Answer', 'Image_id'])  # Write header\n",
    "    writer.writerows(csv_data)\n",
    "\n",
    "print(\"CSV file created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating combined files from both train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files merged successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the first CSV file\n",
    "df1 = pd.read_csv('Final_Dataset/data_train.csv')\n",
    "\n",
    "# Read the second CSV file\n",
    "df2 = pd.read_csv('Final_Dataset/data_eval.csv')\n",
    "\n",
    "# Merge the two dataframes\n",
    "merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Write the merged dataframe to a new CSV file\n",
    "merged_df.to_csv('Final_Dataset/data.csv', index=False)\n",
    "\n",
    "print(\"CSV files merged successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the merged CSV file\n",
    "merged_df = pd.read_csv('Final_Dataset/data.csv')\n",
    "\n",
    "# Extract the \"Question\" and \"Answer\" columns\n",
    "question_answer_data = merged_df[['Question', 'Answer']]\n",
    "\n",
    "# Write the data to a text file\n",
    "with open('Final_Dataset/all_pairs.txt', 'w') as file:\n",
    "    for index, row in question_answer_data.iterrows():\n",
    "        file.write(f\"{row['Question']}\\n\")\n",
    "        file.write(f\"{row['Answer']}\\n\")\n",
    "\n",
    "print(\"Data written\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique answers DataFrame written.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the merged CSV file\n",
    "merged_df = pd.read_csv('Final_Dataset/data.csv')\n",
    "\n",
    "# Extract unique answers as a DataFrame\n",
    "unique_answers_df = pd.DataFrame(merged_df['Answer'].unique(), columns=['Unique_Answers'])\n",
    "\n",
    "# Write unique answers DataFrame to a text file\n",
    "unique_answers_df.to_csv('answers.txt', index=False, header=False, sep='\\n')\n",
    "\n",
    "print(\"Unique answers DataFrame written.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image IDs have been written to Final_Dataset/train_image_ids.txt\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Read the CSV file\n",
    "csv_filename = 'Final_Dataset/data_train.csv'\n",
    "image_ids = set()  # Using a set to avoid duplicates\n",
    "\n",
    "with open(csv_filename, 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    for row in csvreader:\n",
    "        image_id = row[2]  # Assuming the image_id is in the third column\n",
    "        image_ids.add(image_id)\n",
    "\n",
    "# Write the image IDs to a text file\n",
    "txt_filename = 'Final_Dataset/train_image_ids.txt'\n",
    "\n",
    "with open(txt_filename, 'w') as txtfile:\n",
    "    for image_id in sorted(image_ids):  # Sort the IDs if needed\n",
    "        txtfile.write(image_id + '\\n')\n",
    "\n",
    "print(f'Image IDs have been written to {txt_filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image IDs have been written to Final_Dataset/val_image_ids.txt\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Read the CSV file\n",
    "csv_filename = 'Final_Dataset/data_eval.csv'\n",
    "image_ids = set()  # Using a set to avoid duplicates\n",
    "\n",
    "with open(csv_filename, 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    for row in csvreader:\n",
    "        image_id = row[2]  # Assuming the image_id is in the third column\n",
    "        image_ids.add(image_id)\n",
    "\n",
    "# Write the image IDs to a text file\n",
    "txt_filename = 'Final_Dataset/val_image_ids.txt'\n",
    "\n",
    "with open(txt_filename, 'w') as txtfile:\n",
    "    for image_id in sorted(image_ids):  # Sort the IDs if needed\n",
    "        txtfile.write(image_id + '\\n')\n",
    "\n",
    "print(f'Image IDs have been written to {txt_filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
