{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from datasets import load_dataset, set_caching_enabled\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    # Preprocessing / Common\n",
    "    AutoTokenizer, AutoFeatureExtractor,\n",
    "    # Text & Image Models (Now, image transformers like ViTModel, DeiTModel, BEiT can also be loaded using AutoModel)\n",
    "    AutoModel, AutoConfig,            \n",
    "    # Training / Evaluation\n",
    "    TrainingArguments, Trainer,\n",
    "    # Misc\n",
    "    logging\n",
    ")\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import nltk\n",
    "import subprocess\n",
    "\n",
    "# Download and unzip wordnet\n",
    "try:\n",
    "    nltk.data.find('wordnet.zip')\n",
    "except:\n",
    "    nltk.download('wordnet', download_dir='/kaggle/working/')\n",
    "    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n",
    "    subprocess.run(command.split())\n",
    "    nltk.data.path.append('/kaggle/working/')\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultimodalVQAModel()\n",
    "model.load_state_dict(torch.load(latest_checkpoint_path))\n",
    "model.to(device) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "# Randomly sample 10 indices from the test dataset\n",
    "random_sample_indices = random.sample(range(len(dataset[\"test\"])), k=10)\n",
    "\n",
    "# Sample data for manual testing\n",
    "sample = collator([dataset[\"test\"][index] for index in random_sample_indices])\n",
    "\n",
    "# Extract input components from the sample for manual testing\n",
    "input_ids = sample[\"input_ids\"].to(device)\n",
    "token_type_ids = sample[\"token_type_ids\"].to(device)\n",
    "attention_mask = sample[\"attention_mask\"].to(device)\n",
    "pixel_values = sample[\"pixel_values\"].to(device)\n",
    "labels = sample[\"labels\"].to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Forward pass with the sample data\n",
    "output = model(input_ids, pixel_values, attention_mask, token_type_ids, labels)\n",
    "\n",
    "# Get predictions from the model output\n",
    "predictions = output[\"logits\"].argmax(axis=-1).cpu().numpy()\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def similarity(a, b):\n",
    "    # Split words if it is a list and remove extra spaces\n",
    "    words_a = [w.strip() for w in a.split(',')]\n",
    "    words_b = [w.strip() for w in b.split(',')]\n",
    "\n",
    "    # Split words if connected by underscore _\n",
    "    a = [w_ for word in words_a for w_ in word.split('_')]\n",
    "    b = [w_ for word in words_b for w_ in word.split('_')]\n",
    "\n",
    "    res = 0\n",
    "    n = 0\n",
    "\n",
    "    # Calculate score and take average\n",
    "    for i in a:\n",
    "        synsets_i = wordnet.synsets(i)\n",
    "        if synsets_i:\n",
    "            s1 = synsets_i[0]\n",
    "            for j in b:\n",
    "                synsets_j = wordnet.synsets(j)\n",
    "                if synsets_j:\n",
    "                    s2 = synsets_j[0]\n",
    "                    sim = s1.wup_similarity(s2)\n",
    "                    if sim:\n",
    "                        res += sim\n",
    "                    n += 1\n",
    "\n",
    "    return res / n if n != 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show predictions for a range of examples\n",
    "for i in range(2000, 2005):\n",
    "    print(\"\\n=========================================================\\n\")\n",
    "    real_answer = show_example(train=False, idx=i)\n",
    "    predicted_answer = answer_space[preds[i - 2000]]\n",
    "    print(\"Predicted Answer:\\t\", predicted_answer)\n",
    "    print(f\"Similarity: {similarity(real_answer, predicted_answer)}\")\n",
    "    print(\"\\n=========================================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Count and print the number of trainable parameters in a PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The PyTorch model.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Number of trainable parameters: {:,}\".format(num_params))\n",
    "count_trainable_parameters(model)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
